{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取电影评论数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "pbar = pyprind.ProgBar(50000) #生成一个50000次迭代的进度条\n",
    "labels = {'pos':1,'neg':0}\n",
    "df = pd.DataFrame()\n",
    "home_path='/Users/fannian/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对语料库文件夹进行遍历，将测试集test\\训练集train\\正例样本集pos\\负例样本集neg导入dataFrame\n",
    "os.listdir(path) 会返回包含路径path下所有文件的列表\n",
    "os.path.join(path,file) 会返回一个path和file的组合地址(用/连接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:05:28\n"
     ]
    }
   ],
   "source": [
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = home_path+'tool_data/aclImdb/%s/%s' % (s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file), 'r') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()#更新进度条"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.permutation()对数据结构(df)的索引编号进行随机打乱\n",
    "df.reindex(newindex) 用新的索引编号替换原有索引，若原有索引数据不存在，则为NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['review', 'sentiment']\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(home_path+'tool_data/outdata/movie_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本转稀疏向量矩阵、TF-IDF计算单词重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer()#词频：训练后生成一个词频字典\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)#设置numpy显示\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "])\n",
    "bag = count.fit_transform(docs)#生成特征词向量的压缩行稀疏矩阵\n",
    "print(count.vocabulary_) #返回一个词频字典\n",
    "tff = tfidf.fit_transform(bag) #计算词向量矩阵的TFIDF矩阵，并进行归一化处理\n",
    "bag.toarray()#将压缩行系数矩阵转为ndarray格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.56, 0.56, 0.  , 0.43, 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.56, 0.43, 0.56],\n",
       "       [0.4 , 0.48, 0.31, 0.31, 0.31, 0.48, 0.31]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to star cinema way to go jericho and claudine '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(home_path+'tool_data/outdata/movie_data.csv')\n",
    "dr = df.loc[0,'review'][-50:]\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>','',text)#正则表达式将Html符号替换掉\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)#找出表情符号并以列表形式存储\n",
    "    #替换所有非单词字符,将文本转换为小写，并将表情符号追加到文本之后（删除代表鼻子的字符-）\n",
    "    text = re.sub('[\\W]+',' ',text.lower()) + ''.join(emoticons).replace('-','')\n",
    "    return text\n",
    "preprocessor(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :):(:)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标记文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer() #词干提取算法\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    \"\"\"以空格符分割字符串，返回一个单词列表,并对该列表每个元素进行词干提取\"\"\"\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "def tokenizer(text):\n",
    "    \"\"\"以空格符分割字符串，返回一个单词列表\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归应用于文档分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fannian/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89872"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.loc[:,'review'].values\n",
    "y = df.loc[:,'sentiment'].values\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.5)\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "lr = LogisticRegression(C=10.0,penalty='l2')\n",
    "tfidf_vect =  TfidfVectorizer(stop_words=None,tokenizer=tokenizer)\n",
    "nb_tfidf = Pipeline([('vect',tfidf_vect),('clf',lr)])\n",
    "nb_tfidf.fit(X_train,y_train)\n",
    "nb_tfidf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在线算法与外存学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My family\n",
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    '''清理movie_data.csv文件中未经处理的文本'''\n",
    "    text =re.sub('<[^>]*>','',text)\n",
    "    emoticons=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text.lower())\n",
    "    text=re.sub('[\\W]+',' ',text.lower()) + ' '.join(emoticons).replace('-','')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    '''每次读取且返回一个文档的内容'''\n",
    "    with open(path,'r') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text,label = line[:-3],int(line[-2])\n",
    "            yield text,label\n",
    " \n",
    "print (next(stream_docs(path=home_path+'tool_data/outdata/movie_data.csv'))[0][:10])\n",
    "\n",
    "def get_minibatch(doc_stream,size):\n",
    "    '''以stream_doc函数得到的文档数据流作为输入，\n",
    "    并通过参数size返回指定数量的文档内容'''\n",
    "    docs,y=[],[]\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text,label=next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None,None\n",
    "    return docs,y\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',n_features=2**21,\n",
    "                         preprocessor=None,tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log',random_state=1)\n",
    "doc_stream = stream_docs(path=home_path+'tool_data/outdata/movie_data.csv')\n",
    "\n",
    "pbar=pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train,y_train = get_minibatch(doc_stream,size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train,y_train,classes=classes)\n",
    "    pbar.update\n",
    "    \n",
    "X_test,y_test=get_minibatch(doc_stream,size=5000)\n",
    "X_test =vect.transform(X_test)\n",
    "print ('Accuracy: %.3f' % clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列化通过scikit-learn拟合的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dest = os.path.join('movieclassifier','pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "pickle.dump(stop,open(os.path.join(dest,'stopwords.pkl'),'wb'),protocol=4)#protocol协议\n",
    "pickle.dump(clf,open(os.path.join(dest,'classifier.pkl'),'wb'),protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
